{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import statistics\n",
    "import os\n",
    "import getpass\n",
    "from typing import Dict, List, Any, TypedDict, Union, Optional\n",
    "from functools import lru_cache\n",
    "import time\n",
    "from datetime import datetime\n",
    "import threading\n",
    "import queue\n",
    "\n",
    "# Setting up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"restaurant_agent.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Required imports for LangGraph and LLM interaction\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.callbacks.base import BaseCallbackHandler\n",
    "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "\n",
    "class UserPreferences(TypedDict):\n",
    "    cuisine_type: Optional[List[str]]\n",
    "    food_type: Optional[List[str]]\n",
    "    location: str\n",
    "    special_features: Optional[List[str]]  # special requirements (e.g., outdoor dining/area, payment_options, etc.)\n",
    "\n",
    "class ChatState(TypedDict): # ChatState is a custom dictionary type(TypedDict) that defines the structure of the chatbot's state \n",
    "    messages: List[Union[HumanMessage, AIMessage, SystemMessage]] # list of messages exchanged in the chat \n",
    "    intent: Optional[str] # intent of the user query\n",
    "    user_preferences: UserPreferences \n",
    "    specific_restaurant: Optional[List[str]] # multiple restaurants can be mentioned in the user query while asking for restaurant information(only help for restaurant_info query type).\n",
    "    restaurant_matches: Optional[List[Dict[str, Any]]] # list of restaurant options that match the user's query\n",
    "    conversation_history: Optional[List[Dict[str, Any]]] # record of past conversations(for continuity)\n",
    "    session_id: Optional[str] # unique identifier for the chat session\n",
    "\n",
    "# Define custom callback handler for streaming\n",
    "class StreamingCallbackHandler(BaseCallbackHandler): # class that handles streaming responses from the AI model\n",
    "    def __init__(self, queue): # the class takes queue as an argument to store the generated tokens(one by one)\n",
    "        self.queue = queue # queue is used to stream responses in real-time\n",
    "        \n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None: # called whenever the AI generates a new token(word/phrase) which is then added to the queue\n",
    "        self.queue.put(token)\n",
    "\n",
    "\n",
    "# Global LLM cache\n",
    "LLM_CACHE = {} # llm_cache is a dictionary that stores AI model instances so they can be reused instead of loding a new model every time(reduces latency).\n",
    "\n",
    "# Initializes and retrieves the AI language model\n",
    "def get_llm(temperature=0.2, streaming=False, queue=None):\n",
    "    cache_key = f\"llm_{temperature}_{streaming}\"\n",
    "    if cache_key in LLM_CACHE:\n",
    "        return LLM_CACHE[cache_key]\n",
    "    \n",
    "    callbacks = []\n",
    "    if streaming and queue: # If streaming is enabled and a queue is provided, a StreamingCallbackHandler is added to handle token-by-token responses.\n",
    "        callbacks.append(StreamingCallbackHandler(queue))\n",
    "    \n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=temperature,\n",
    "        api_key=OPENAI_API_KEY,\n",
    "        streaming=streaming,\n",
    "        callbacks=callbacks if callbacks else None\n",
    "    )\n",
    "    \n",
    "    LLM_CACHE[cache_key] = llm\n",
    "    return llm\n",
    "\n",
    "# configure logging throughout the code\n",
    "# create backend and frontend for both normal and streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of ratings: 17\n",
      "Mean Rating: 4.3352941176470585\n",
      "Median Rating: 4.5\n",
      "Mode Rating: 4.6\n"
     ]
    }
   ],
   "source": [
    "with open(\"100_restaurant_data.json\", \"r\") as file:\n",
    "    restaurants = json.load(file)\n",
    "\n",
    "# Extract all ratings from the JSON data\n",
    "ratings = [restaurant[\"rating\"] for restaurant in restaurants if \"rating\" in restaurant and isinstance(restaurant[\"rating\"], (int, float))]\n",
    "\n",
    "# Calculate mean, median, and mode\n",
    "mean_rating = statistics.mean(ratings)\n",
    "median_rating = statistics.median(ratings)\n",
    "mode_rating = statistics.mode(ratings)  \n",
    "# calculating the measures of dispersion so that qualitative data(good, nice, best etc) can be converted to quantitative data\n",
    "\n",
    "print(f\"Total number of ratings: {len(ratings)}\")\n",
    "print(f\"Mean Rating: {mean_rating}\")\n",
    "print(f\"Median Rating: {median_rating}\")\n",
    "\n",
    "# Handling multiple modes\n",
    "try:\n",
    "    print(f\"Mode Rating: {mode_rating}\")\n",
    "except statistics.StatisticsError:\n",
    "    mode_ratings = statistics.multimode(ratings)\n",
    "    print(f\"Mode Ratings: {mode_ratings}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "print(OPENAI_API_KEY) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=1) # decorator caches the function results to avoid reloading the restaurant data multiple times, maxsize=1 ensures that only the most recent dataset is stored in the cache(useful for efficiency)  \n",
    "def load_restaurants(json_file_path: str) -> List[Dict[str, Any]]: # reads a JSON file containing restaurant data\n",
    "    try:\n",
    "        logger.info(f\"Loading restaurant data from {json_file_path}\")\n",
    "        with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "            restaurants = json.load(file)\n",
    "        logger.info(f\"Successfully loaded {len(restaurants)} restaurants from the database\")\n",
    "        return restaurants\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading restaurant data: {e}\", exc_info=True)\n",
    "        return []\n",
    "\n",
    "\n",
    "# Function to prepare restaurant documents for vector store\n",
    "def prepare_restaurant_docs(restaurants: List[Dict[str, Any]]) -> List[Dict[str, Any]]: # takes a list of restaurant dictionaries and converts them into a structured format for storage in a vector db\n",
    "    logger.info(f\"Preparing document representations for {len(restaurants)} restaurants\")\n",
    "    docs = []\n",
    "\n",
    "    for i, restaurant in enumerate(restaurants):\n",
    "        if i % 50 == 0 and i > 0:\n",
    "            logger.debug(f\"Processed {i} restaurants so far\")\n",
    "        \n",
    "        text_content = f\"Restaurant Name: {restaurant.get('name', '')}\\n\" # Creates a comprehensive text representation of each restaurant\n",
    "        \n",
    "        # Location information\n",
    "        city = restaurant.get('city', '')\n",
    "        state = restaurant.get('state', '')\n",
    "        neighborhood = restaurant.get('neighborhood', '')\n",
    "        address = restaurant.get('street_address', '')\n",
    "        zipcode = restaurant.get('zipcode', '')\n",
    "        country = restaurant.get('country', '')\n",
    "        cross_street = restaurant.get('cross_street', '')\n",
    "        \n",
    "        location_parts = []\n",
    "        if address:\n",
    "            location_parts.append(address)\n",
    "        if neighborhood:\n",
    "            location_parts.append(f\"Neighborhood: {neighborhood}\")\n",
    "        if cross_street:\n",
    "            location_parts.append(f\"Cross Street: {cross_street}\")\n",
    "        if city:\n",
    "            location_parts.append(city)\n",
    "        if state:\n",
    "            location_parts.append(state)\n",
    "        if country:\n",
    "            location_parts.append(country)\n",
    "        if zipcode:\n",
    "            location_parts.append(zipcode)\n",
    "        \n",
    "        location_str = \", \".join(location_parts)\n",
    "        text_content += f\"Location: {location_str}\\n\"\n",
    "        \n",
    "        # Rating and reviews\n",
    "        rating = restaurant.get('rating')\n",
    "        review_count = restaurant.get('review_count')\n",
    "        if rating is not None:\n",
    "            text_content += f\"Rating: {rating}\"\n",
    "        if review_count is not None:\n",
    "            text_content += f\" (from {review_count} reviews)\"\n",
    "            text_content += \"\\n\"\n",
    "        \n",
    "        # Price information\n",
    "        price = restaurant.get('price')\n",
    "        payment_options = restaurant.get('payment_options', [])\n",
    "        if price is not None:\n",
    "            text_content += f\"Price Level: {price}\\n\"\n",
    "        if payment_options:\n",
    "            text_content += f\"Payment Options: {', '.join(payment_options)}\\n\"\n",
    "        \n",
    "        # Cuisines\n",
    "        cuisines = restaurant.get('cuisines', [])\n",
    "        if cuisines:\n",
    "            text_content += f\"Cuisines: {', '.join(cuisines)}\\n\"\n",
    "        \n",
    "        # Tags (for additional food types, ambiance, etc.)\n",
    "        tags = restaurant.get('tags', [])\n",
    "        if tags:\n",
    "            text_content += f\"Tags: {', '.join(tags)}\\n\"\n",
    "        \n",
    "        # Popular dishes\n",
    "        popular_dishes = restaurant.get('popular_dishes', [])\n",
    "        if popular_dishes:\n",
    "            text_content += f\"Popular Dishes: {', '.join(popular_dishes)}\\n\"\n",
    "        \n",
    "        # Description or endorsement\n",
    "        description = restaurant.get('description')\n",
    "        endorsement = restaurant.get('endorsement_copy')\n",
    "        if description:\n",
    "            text_content += f\"Description: {description}\\n\"\n",
    "        elif endorsement:\n",
    "            text_content += f\"Description: {endorsement}\\n\"\n",
    "        \n",
    "        # Featured in publications\n",
    "        featured_in = restaurant.get('featured_in')\n",
    "        if featured_in:\n",
    "            text_content += f\"Featured in: {featured_in}\\n\"\n",
    "        \n",
    "        # Contact details\n",
    "        phone_number = restaurant.get('phone_number', '')\n",
    "        restaurant_url = restaurant.get('restaurant_url', '')  # Fixed typo here\n",
    "        if phone_number and restaurant_url:\n",
    "            text_content += f\"Phone number is {phone_number} and restaurant url is {restaurant_url}.\"\n",
    "        elif phone_number:\n",
    "            text_content += f\"Phone number is {phone_number}.\"\n",
    "        elif restaurant_url:\n",
    "            text_content += f\"The restaurant url is {restaurant_url}.\"\n",
    "        \n",
    "        # Additional amenities\n",
    "        if restaurant.get('reservations_required') is True:\n",
    "            text_content += \"Reservations required.\\n\"\n",
    "        \n",
    "        if restaurant.get('dining_style'):\n",
    "            text_content += f\"Dining style: {restaurant.get('dining_style')}\\n\"\n",
    "        \n",
    "        if restaurant.get('parking_details'):\n",
    "            text_content += f\"Parking: {restaurant.get('parking_details')}\\n\"\n",
    "        \n",
    "        if restaurant.get('public_transport'):\n",
    "            text_content += f\"Public transport: {restaurant.get('public_transport')}\\n\"\n",
    "        \n",
    "        # Create document for vectorstore with rich metadata\n",
    "        metadata = {\n",
    "            \"id\": restaurant.get(\"id\") if restaurant.get(\"id\") else None,\n",
    "            \"name\": restaurant.get(\"name\") if restaurant.get(\"name\") else None,\n",
    "            \"location\": location_str,\n",
    "            \"price\": restaurant.get(\"price\") if restaurant.get(\"price\") else None,\n",
    "            \"restaurant_url\": restaurant.get(\"restaurant_url\") if restaurant.get(\"restaurant_url\") else None,\n",
    "            \"images_url\" : restaurant.get(\"images_url\") if restaurant.get(\"images_url\") else None,\n",
    "            \"coordinates\": restaurant.get(\"location_geom\", {}).get(\"coordinates\") if restaurant.get(\"location_geom\") else None,\n",
    "            \"original_data\": restaurant\n",
    "        }\n",
    "        docs.append(Document(page_content=text_content, metadata=metadata))\n",
    "    \n",
    "    logger.info(f\"Finished preparing {len(docs)} restaurant documents\")\n",
    "    return docs\n",
    "\n",
    "def save_faiss_index(vector_store, directory_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Save a FAISS vector store to disk.\n",
    "    \n",
    "    Args:\n",
    "        vector_store: The FAISS vector store to save\n",
    "        directory_path: The directory path where the index will be saved\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Saving FAISS index to {directory_path}\")\n",
    "        vector_store.save_local(directory_path)\n",
    "        logger.info(f\"Successfully saved FAISS index to {directory_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving FAISS index: {e}\", exc_info=True)\n",
    "\n",
    "def load_faiss_index(directory_path: str, embedding_model=None) -> FAISS:\n",
    "    \"\"\"\n",
    "    Load a FAISS vector store from disk.\n",
    "    \n",
    "    Args:\n",
    "        directory_path: The directory path where the index is stored\n",
    "        embedding_model: The embedding model to use (optional if saved with the index)\n",
    "    \n",
    "    Returns:\n",
    "        A FAISS vector store\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Loading FAISS index from {directory_path}\")\n",
    "        if embedding_model is None:\n",
    "            embedding_model = OpenAIEmbeddings(model=EMBEDDING_MODEL)\n",
    "            logger.debug(\"Created new embedding model instance\")\n",
    "        \n",
    "        vector_store = FAISS.load_local(directory_path, embedding_model, allow_dangerous_deserialization=True)\n",
    "        logger.info(f\"Successfully loaded FAISS index from {directory_path}\")\n",
    "        return vector_store\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading FAISS index: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def setup_retriever_with_persistence(restaurants_json_path: str, index_dir: str = \"faiss_index\") -> VectorStoreRetriever:\n",
    "    \"\"\"\n",
    "    Sets up a retriever using a persisted FAISS index if available, otherwise creates and saves a new index.\n",
    "    \n",
    "    Args:\n",
    "        restaurants_json_path: Path to the JSON file containing restaurant data\n",
    "        index_dir: Directory to save/load the FAISS index\n",
    "    \n",
    "    Returns:\n",
    "        A VectorStoreRetriever\n",
    "    \"\"\"\n",
    "    # Check if index exists\n",
    "    if os.path.exists(index_dir) and os.path.isdir(index_dir):\n",
    "        logger.info(f\"Found existing FAISS index at {index_dir}\")\n",
    "        embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)\n",
    "        vector_store = load_faiss_index(index_dir, embeddings)\n",
    "        \n",
    "        # If loading failed, create a new index\n",
    "        if vector_store is None:\n",
    "            logger.warning(\"Failed to load existing index. Creating a new one...\")\n",
    "            vector_store = create_and_save_index(restaurants_json_path, index_dir)\n",
    "    else:\n",
    "        logger.info(f\"No existing index found at {index_dir}. Creating a new one...\")\n",
    "        vector_store = create_and_save_index(restaurants_json_path, index_dir)\n",
    "    \n",
    "    # Create and return the retriever\n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 5}  # Return top 5 matches\n",
    "    )\n",
    "\n",
    "    logger.info(\"Created and configured vector store retriever\")\n",
    "    return retriever\n",
    "\n",
    "def create_and_save_index(restaurants_json_path: str, index_dir: str) -> FAISS:\n",
    "    \"\"\"\n",
    "    Creates a new FAISS index from restaurant data and saves it to disk.\n",
    "    \n",
    "    Args:\n",
    "        restaurants_json_path: Path to the JSON file containing restaurant data\n",
    "        index_dir: Directory to save the FAISS index\n",
    "    \n",
    "    Returns:\n",
    "        A FAISS vector store\n",
    "    \"\"\"\n",
    "    logger.info(f\"Creating new FAISS index from {restaurants_json_path}\")\n",
    "    restaurants = load_restaurants(restaurants_json_path)\n",
    "    logger.debug(f\"Loaded {len(restaurants)} restaurants\")\n",
    "    \n",
    "    docs = prepare_restaurant_docs(restaurants)\n",
    "    logger.debug(f\"Prepared {len(docs)} documents\")\n",
    "    \n",
    "    logger.info(\"Creating embedding model and vector store\")\n",
    "    try:\n",
    "        embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)\n",
    "        vector_store = FAISS.from_documents(docs, embeddings) # A single restaurant data is completely stored in a single chunk\n",
    "        logger.info(\"Successfully created FAISS vector store\")\n",
    "        \n",
    "        # Save the index\n",
    "        save_faiss_index(vector_store, index_dir)\n",
    "        \n",
    "        return vector_store\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating vector store: {e}\", exc_info=True)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 22:18:35,430 - __main__ - INFO - Initialized ConversationMemory with max_sessions=50, max_history=15\n",
      "2025-03-23 22:18:35,432 - __main__ - INFO - Global ConversationMemory initialized\n"
     ]
    }
   ],
   "source": [
    "# Memory manager for conversation history\n",
    "class ConversationMemory: # used to store and manage chat history b/w a user and a chatbot \n",
    "    def __init__(self, max_sessions=50, max_history_per_session=15):\n",
    "        self.sessions = {} # stores conversations per session\n",
    "        self.max_sessions = max_sessions\n",
    "        self.max_history_per_session = max_history_per_session # stores up to 10 messages per session\n",
    "        logger.info(f\"Initialized ConversationMemory with max_sessions={max_sessions}, max_history={max_history_per_session}\")\n",
    "        \n",
    "    def add_interaction(self, session_id, user_message, bot_response, metadata=None): # adds a new user-bot conversation to memory\n",
    "        if session_id not in self.sessions:\n",
    "            self.sessions[session_id] = []\n",
    "            logger.info(f\"Created new session: {session_id}\")\n",
    "            \n",
    "        # Limit sessions\n",
    "        if len(self.sessions) > self.max_sessions:\n",
    "            oldest_session = min(self.sessions.keys(), key=lambda k: self.sessions[k][0]['timestamp'] if self.sessions[k] else datetime.now().timestamp())\n",
    "            logger.info(f\"Session limit reached. Removing oldest session: {oldest_session}\")\n",
    "            del self.sessions[oldest_session]\n",
    "        \n",
    "        # Add the new interaction\n",
    "        interaction = {\n",
    "            'timestamp': datetime.now().timestamp(),\n",
    "            'user_message': user_message,\n",
    "            'bot_response': bot_response,\n",
    "            'metadata': metadata or {}\n",
    "        }\n",
    "        \n",
    "        self.sessions[session_id].append(interaction)\n",
    "        logger.debug(f\"Added interaction to session {session_id}. Message length: User={len(user_message)}, Bot={len(bot_response)}\")\n",
    "        \n",
    "        # Trim history if needed\n",
    "        if len(self.sessions[session_id]) > self.max_history_per_session:\n",
    "            logger.debug(f\"Trimming history for session {session_id}\")\n",
    "            self.sessions[session_id] = self.sessions[session_id][-self.max_history_per_session:]\n",
    "    \n",
    "    def get_history(self, session_id, limit=None): # returns past messages for a given session_id\n",
    "        if session_id not in self.sessions:\n",
    "            logger.debug(f\"No history found for session {session_id}\")\n",
    "            return []\n",
    "        \n",
    "        history = self.sessions[session_id]\n",
    "        if limit:\n",
    "            logger.debug(f\"Returning {min(limit, len(history))} history items for session {session_id}\")\n",
    "            return history[-limit:]\n",
    "        \n",
    "        logger.debug(f\"Returning all {len(history)} history items for session {session_id}\")\n",
    "        return history\n",
    "\n",
    "\n",
    "# Create global conversation memory\n",
    "CONVERSATION_MEMORY = ConversationMemory() # to store all user conversations\n",
    "logger.info(\"Global ConversationMemory initialized\")\n",
    "\n",
    "# Query cache\n",
    "QUERY_CACHE = {} # dictionary to store cached search results \n",
    "QUERY_CACHE_LOCK = threading.Lock() # A lock to prevent multiple users from modifying the cache at the same time\n",
    "\n",
    "def get_cached_response(query_key):\n",
    "    with QUERY_CACHE_LOCK:\n",
    "        if query_key in QUERY_CACHE:\n",
    "            logger.debug(f\"Cache hit for key: {query_key[:50]}...\")\n",
    "            return QUERY_CACHE.get(query_key)\n",
    "        logger.debug(f\"Cache miss for key: {query_key[:50]}...\")\n",
    "        return None\n",
    "\n",
    "def set_cached_response(query_key, response): # stores a query result in the cache \n",
    "    with QUERY_CACHE_LOCK:\n",
    "        QUERY_CACHE[query_key] = response\n",
    "        logger.debug(f\"Cached response for key: {query_key[:50]}...\")\n",
    "\n",
    "        # Limit cache size to 100 entries\n",
    "        if len(QUERY_CACHE) > 100:\n",
    "            oldest_key = next(iter(QUERY_CACHE)) # Removes oldest entry\n",
    "            logger.info(f\"Cache limit reached. Removing oldest entry: {oldest_key[:50]}...\")\n",
    "            del QUERY_CACHE[oldest_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 22:18:35,474 - __main__ - INFO - Initialized ConversationMemory with max_sessions=50, max_history=15\n",
      "2025-03-23 22:18:35,476 - __main__ - INFO - Global ConversationMemory initialized\n"
     ]
    }
   ],
   "source": [
    "# Memory manager for conversation history\n",
    "class ConversationMemory: # used to store and manage chat history b/w a user and a chatbot \n",
    "    def __init__(self, max_sessions=50, max_history_per_session=15):\n",
    "        self.sessions = {} # stores conversations per session\n",
    "        self.max_sessions = max_sessions\n",
    "        self.max_history_per_session = max_history_per_session # stores up to 10 messages per session\n",
    "        logger.info(f\"Initialized ConversationMemory with max_sessions={max_sessions}, max_history={max_history_per_session}\")\n",
    "        \n",
    "    def add_interaction(self, session_id, user_message, bot_response, metadata=None): # adds a new user-bot conversation to memory\n",
    "        if session_id not in self.sessions:\n",
    "            self.sessions[session_id] = []\n",
    "            logger.info(f\"Created new session: {session_id}\")\n",
    "            \n",
    "        # Limit sessions\n",
    "        if len(self.sessions) > self.max_sessions:\n",
    "            oldest_session = min(self.sessions.keys(), key=lambda k: self.sessions[k][0]['timestamp'] if self.sessions[k] else datetime.now().timestamp())\n",
    "            logger.info(f\"Session limit reached. Removing oldest session: {oldest_session}\")\n",
    "            del self.sessions[oldest_session]\n",
    "        \n",
    "        # Add the new interaction\n",
    "        interaction = {\n",
    "            'timestamp': datetime.now().timestamp(),\n",
    "            'user_message': user_message,\n",
    "            'bot_response': bot_response,\n",
    "            'metadata': metadata or {}\n",
    "        }\n",
    "        \n",
    "        self.sessions[session_id].append(interaction)\n",
    "        logger.debug(f\"Added interaction to session {session_id}. Message length: User={len(user_message)}, Bot={len(bot_response)}\")\n",
    "        \n",
    "        # Trim history if needed\n",
    "        if len(self.sessions[session_id]) > self.max_history_per_session:\n",
    "            logger.debug(f\"Trimming history for session {session_id}\")\n",
    "            self.sessions[session_id] = self.sessions[session_id][-self.max_history_per_session:]\n",
    "    \n",
    "    def get_history(self, session_id, limit=None): # returns past messages for a given session_id\n",
    "        if session_id not in self.sessions:\n",
    "            logger.debug(f\"No history found for session {session_id}\")\n",
    "            return []\n",
    "        \n",
    "        history = self.sessions[session_id]\n",
    "        if limit:\n",
    "            logger.debug(f\"Returning {min(limit, len(history))} history items for session {session_id}\")\n",
    "            return history[-limit:]\n",
    "        \n",
    "        logger.debug(f\"Returning all {len(history)} history items for session {session_id}\")\n",
    "        return history\n",
    "\n",
    "\n",
    "# Create global conversation memory\n",
    "CONVERSATION_MEMORY = ConversationMemory() # to store all user conversations\n",
    "logger.info(\"Global ConversationMemory initialized\")\n",
    "\n",
    "# Query cache\n",
    "QUERY_CACHE = {} # dictionary to store cached search results \n",
    "QUERY_CACHE_LOCK = threading.Lock() # A lock to prevent multiple users from modifying the cache at the same time\n",
    "\n",
    "def get_cached_response(query_key):\n",
    "    with QUERY_CACHE_LOCK:\n",
    "        if query_key in QUERY_CACHE:\n",
    "            logger.debug(f\"Cache hit for key: {query_key[:50]}...\")\n",
    "            return QUERY_CACHE.get(query_key)\n",
    "        logger.debug(f\"Cache miss for key: {query_key[:50]}...\")\n",
    "        return None\n",
    "\n",
    "def set_cached_response(query_key, response): # stores a query result in the cache \n",
    "    with QUERY_CACHE_LOCK:\n",
    "        QUERY_CACHE[query_key] = response\n",
    "        logger.debug(f\"Cached response for key: {query_key[:50]}...\")\n",
    "\n",
    "        # Limit cache size to 100 entries\n",
    "        if len(QUERY_CACHE) > 100:\n",
    "            oldest_key = next(iter(QUERY_CACHE)) # Removes oldest entry\n",
    "            logger.info(f\"Cache limit reached. Removing oldest entry: {oldest_key[:50]}...\")\n",
    "            del QUERY_CACHE[oldest_key]\n",
    "\n",
    "\n",
    "def analyze_user_query(state: ChatState) -> ChatState:\n",
    "    \"\"\"\n",
    "    Combined function to analyze the latest user query:\n",
    "    1. Classifies the intent into restaurant_recommendation, specific_restaurant_info, or casual_conversation\n",
    "    2. Extracts relevant information like cuisine type, location, price, etc.\n",
    "    \n",
    "    Args:\n",
    "        state: The current chat state containing messages and other context\n",
    "        \n",
    "    Returns:\n",
    "        Updated state with intent classification and extracted information\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1].content if messages and isinstance(messages[-1], HumanMessage) else \"\"\n",
    "    session_id = state.get(\"session_id\", \"unknown_session\")\n",
    "    \n",
    "    logger.info(f\"Analyzing user query for session {session_id}\")\n",
    "    logger.debug(f\"User query: {last_message[:100]}...\")\n",
    "    \n",
    "    # Checks cache first for both intent and info extraction\n",
    "    cache_key = f\"analysis_{last_message}\"\n",
    "    cached_analysis = get_cached_response(cache_key)\n",
    "    if cached_analysis:\n",
    "        logger.info(\"Using cached analysis result\")\n",
    "        state.update(cached_analysis)\n",
    "        return state\n",
    "    \n",
    "    # Get conversation history for context\n",
    "    conversation_history = []\n",
    "    if \"session_id\" in state and state[\"session_id\"]:\n",
    "        logger.debug(f\"Retrieving conversation history for session {session_id}\")\n",
    "        history = CONVERSATION_MEMORY.get_history(state[\"session_id\"], limit=5)\n",
    "        conversation_history = [\n",
    "            {\"user\": item[\"user_message\"], \"bot\": item[\"bot_response\"]} \n",
    "            for item in history\n",
    "        ]\n",
    "    \n",
    "    history_context = \"\\n\".join([\n",
    "        f\"User: {item['user']}\\nBot: {item['bot']}\" \n",
    "        for item in conversation_history\n",
    "    ])\n",
    "    \n",
    "    # Create a prompt for combined intent and information extraction\n",
    "    logger.debug(\"Creating prompt for intent classification and info extraction\")\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        SystemMessage(content=f\"\"\"\n",
    "        Analyze the user's message for a restaurant chatbot by:\n",
    "\n",
    "        1. CLASSIFYING THE INTENT into exactly one of these categories:\n",
    "           - restaurant_recommendation: User is looking for restaurant suggestions\n",
    "           - specific_restaurant_info: User is asking about a specific restaurant or set of restaurants\n",
    "           - casual_conversation: General greetings, farewells, or off-topic conversation\n",
    "\n",
    "        2. EXTRACTING INFORMATION relevant to their request (include only if mentioned or implied):\n",
    "           - cuisine_type: Type of cuisine (e.g., Chinese, Italian, Japanese)\n",
    "           - food_type: Specific food or dish (e.g., pasta, sushi, pizza)\n",
    "           - location: City, neighborhood, area, address, cross street, country etc.\n",
    "           - special_features: Any special requirements (e.g., outdoor dining/areaseating, payment options etc.)\n",
    "           - restaurant_name: List of \"Names\" of specific restaurants if mentioned, only implies when the INTENT of the query is specific_restaurant_info \n",
    "\n",
    "        Be interpretive - if user says \"nice Italian place in NYC\", extract the cuisine_type(Italian), location(NYC) and a rating(nice).\n",
    "        \n",
    "        Recent conversation history (consider this for context):\n",
    "        {history_context}\n",
    "        \n",
    "        Respond with a JSON object containing both \"intent\" and \"extracted_info\" fields.\n",
    "        \n",
    "        Example response format:\n",
    "        {{\n",
    "            \"intent\": \"restaurant_recommendation\" OR \"specific_restaurant_info\" OR \"casual_conversation\",\n",
    "            \"extracted_info\": {{\n",
    "                \"cuisine_type\": [\"list of cuisines mentioned or empty list\"],\n",
    "                \"food_type\": [\"list of food types mentioned or empty list\"],\n",
    "                \"location\": \"location mentioned or empty string if none\",\n",
    "                \"special_features\": [\"list of special features mentioned or empty list\"],\n",
    "                \"restaurant_names\": [\"list of restaurant names mentioned or empty list\"],\n",
    "            }}\n",
    "        }}\n",
    "        \n",
    "        Only include fields that are explicitly mentioned or clearly implied in the user's message and return a strictly JSON response with no additional text as shown above in the response format.\n",
    "        \"\"\"),\n",
    "        HumanMessage(content=last_message)\n",
    "    ])\n",
    "    \n",
    "    # Using the LLM to analyze the query\n",
    "    logger.info(\"Sending query to LLM for analysis\")\n",
    "    llm = get_llm(temperature=0)\n",
    "    parser = JsonOutputParser()\n",
    "    chain = prompt | llm | parser\n",
    "    \n",
    "    try:\n",
    "        result = chain.invoke({})\n",
    "        logger.debug(f\"Received analysis result: {result}\")\n",
    "        \n",
    "        state[\"intent\"] = result.get(\"intent\", \"casual_conversation\")\n",
    "        logger.info(f\"Classified intent: {state['intent']}\")\n",
    "        \n",
    "        extracted_info = result.get(\"extracted_info\", {})\n",
    "        logger.debug(f\"Extracted information: {extracted_info}\")\n",
    "        \n",
    "        # Initialize or update user_preferences\n",
    "        if \"user_preferences\" not in state:\n",
    "            state[\"user_preferences\"] = {\n",
    "                \"cuisine_type\": [],\n",
    "                \"food_type\": [],\n",
    "                \"location\": \"\",\n",
    "                \"special_features\": []\n",
    "            }\n",
    "        \n",
    "        # Update user preferences with extracted information\n",
    "        if \"cuisine_type\" in extracted_info and extracted_info[\"cuisine_type\"]:\n",
    "            state[\"user_preferences\"][\"cuisine_type\"] = extracted_info[\"cuisine_type\"]\n",
    "        \n",
    "        if \"food_type\" in extracted_info and extracted_info[\"food_type\"]:\n",
    "            state[\"user_preferences\"][\"food_type\"] = extracted_info[\"food_type\"]\n",
    "        \n",
    "        if \"location\" in extracted_info and extracted_info[\"location\"]:\n",
    "            state[\"user_preferences\"][\"location\"] = extracted_info[\"location\"]\n",
    "        \n",
    "        if \"special_features\" in extracted_info and extracted_info[\"special_features\"]:\n",
    "            state[\"user_preferences\"][\"special_features\"] = extracted_info[\"special_features\"]\n",
    "        \n",
    "        # Handle restaurant names for specific_restaurant_info intent\n",
    "        if state[\"intent\"] == \"specific_restaurant_info\" and \"restaurant_names\" in extracted_info:\n",
    "            state[\"specific_restaurant\"] = extracted_info.get(\"restaurant_names\", [])\n",
    "            logger.debug(f\"Set specific restaurant: {state['specific_restaurant']}\")\n",
    "        \n",
    "        # Cache the analysis for future use\n",
    "        set_cached_response(cache_key, {\n",
    "            \"intent\": state[\"intent\"],\n",
    "            \"user_preferences\": state[\"user_preferences\"],\n",
    "            \"specific_restaurant\": state.get(\"specific_restaurant\", None)\n",
    "        })\n",
    "        logger.info(\"Analysis complete and cached\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Logs error and continues with default values\n",
    "        logger.error(f\"Error parsing LLM response: {e}\", exc_info=True)\n",
    "        state[\"intent\"] = \"casual_conversation\"\n",
    "        logger.info(\"Defaulting to casual_conversation intent due to error\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "def route_query(state: ChatState) -> str:\n",
    "    \"\"\"\n",
    "    Routes the query to the appropriate handler based on intent\n",
    "    \n",
    "    Args:\n",
    "        state: The current chat state containing intent and other context\n",
    "        \n",
    "    Returns:\n",
    "        String indicating which node to route to\n",
    "    \"\"\"\n",
    "    intent = state.get(\"intent\", \"casual_conversation\")\n",
    "    session_id = state.get(\"session_id\", \"unknown_session\")\n",
    "    \n",
    "    logger.info(f\"Routing query for session {session_id} with intent: {intent}\")\n",
    "    \n",
    "    if intent == \"restaurant_recommendation\":\n",
    "        logger.debug(\"Routing to restaurant_recommendation handler\")\n",
    "        return \"restaurant_recommendation\"\n",
    "    elif intent == \"specific_restaurant_info\":\n",
    "        logger.debug(\"Routing to restaurant_info handler\")\n",
    "        return \"restaurant_info\"\n",
    "    else:\n",
    "        logger.debug(\"Routing to casual_conversation handler\")\n",
    "        return \"casual_conversation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_restaurant_recommendation(state: ChatState) -> ChatState:\n",
    "    \"\"\"\n",
    "    Handles restaurant recommendation queries by searching the vector database\n",
    "    and returning matching restaurants, with filtering based on extended criteria\n",
    "    \n",
    "    Args:\n",
    "        state: The current chat state\n",
    "        \n",
    "    Returns:\n",
    "        Updated state with restaurant recommendations\n",
    "    \"\"\"\n",
    "    \n",
    "    session_id = state.get(\"session_id\", \"unknown_session\")\n",
    "    logger.info(f\"Processing restaurant recommendation for session {session_id}\")\n",
    "    \n",
    "    search_criteria = state.get(\"user_preferences\", {})\n",
    "    logger.debug(f\"Search criteria: {search_criteria}\")\n",
    "    \n",
    "    # Build a rich query from the search criteria\n",
    "    query_parts = []\n",
    "    last_message = state[\"messages\"][-1].content if state[\"messages\"] else \"\"\n",
    "    query_parts.append(last_message)\n",
    "    \n",
    "    # Add specific criteria\n",
    "    if search_criteria.get(\"cuisine_type\"):\n",
    "        cuisines = search_criteria[\"cuisine_type\"]\n",
    "        if isinstance(cuisines, list):\n",
    "            query_parts.append(f\"Cuisine types: {', '.join(cuisines)}\")\n",
    "        else:\n",
    "            query_parts.append(f\"Cuisine type: {cuisines}\")\n",
    "    \n",
    "    if search_criteria.get(\"food_type\"):\n",
    "        food_types = search_criteria[\"food_type\"]\n",
    "        if isinstance(food_types, list):\n",
    "            query_parts.append(f\"Food types: {', '.join(food_types)}\")\n",
    "        else:\n",
    "            query_parts.append(f\"Food type: {food_types}\")\n",
    "    \n",
    "    if search_criteria.get(\"location\"):\n",
    "        query_parts.append(f\"Location: {search_criteria['location']}\")\n",
    "    \n",
    "    if search_criteria.get(\"special_features\"):\n",
    "        special_features = search_criteria[\"special_features\"]\n",
    "        if isinstance(special_features, list):\n",
    "            query_parts.append(f\"Special features: {', '.join(special_features)}\")\n",
    "        else:\n",
    "            query_parts.append(f\"Special feature: {special_features}\")\n",
    "    \n",
    "    search_query = \" \".join(query_parts) # Builds the complete query\n",
    "    logger.info(f\"Built search query: {search_query[:100]}...\")\n",
    "    \n",
    "    retriever = setup_retriever_with_persistence(r\"C:\\Users\\Rithwik Khera\\OneDrive - iitr.ac.in\\Desktop\\assignment\\zeal\\100_restaurant_data.json\", r\"C:\\Users\\Rithwik Khera\\OneDrive - iitr.ac.in\\Desktop\\assignment\\zeal\\restaurant_idx\")\n",
    "    logger.debug(\"Retriever setup complete\")\n",
    "\n",
    "    # Search for matching restaurants\n",
    "    # Check cache first\n",
    "    cache_key = f\"recommendation_{search_query}\"\n",
    "    cached_matches = get_cached_response(cache_key)\n",
    "    \n",
    "    if cached_matches:\n",
    "        logger.info(\"Using cached restaurant matches\")\n",
    "        all_matches = cached_matches\n",
    "    else:\n",
    "        # Perform the search\n",
    "        logger.info(\"Performing vector search for restaurants\")\n",
    "        try:\n",
    "            # Retrieve 5 results from vector search\n",
    "            results = retriever.invoke(search_query, top_k=5)\n",
    "            logger.debug(f\"Retrieved {len(results)} results from vector search\")\n",
    "\n",
    "            # Tracking unique restaurant IDs to avoid duplicates using set data structure\n",
    "            seen_restaurant_ids = set()\n",
    "            unique_matches = []\n",
    "\n",
    "            for doc in results:\n",
    "                metadata = doc.metadata\n",
    "                restaurant_id = metadata.get(\"id\", \"\")\n",
    "                \n",
    "                # Only add this restaurant if we haven't seen it before\n",
    "                if restaurant_id and restaurant_id not in seen_restaurant_ids:\n",
    "                    seen_restaurant_ids.add(restaurant_id)\n",
    "                    unique_matches.append({\n",
    "                        \"name\": metadata.get(\"name\", \"Unknown Restaurant\"),\n",
    "                        \"id\": restaurant_id,\n",
    "                        \"content\": doc.page_content,\n",
    "                        \"price\": metadata.get(\"price\"),\n",
    "                        \"restaurant_url\": metadata.get(\"restaurant_url\"),\n",
    "                        \"images_url\": metadata.get(\"images_url\"),\n",
    "                        \"coordinates\": metadata.get(\"coordinates\"),\n",
    "                        \"original_data\": metadata.get(\"original_data\", {})\n",
    "                    })\n",
    "                    \n",
    "                    # Stop after finding 3 unique restaurants\n",
    "                    if len(unique_matches) >= 3:\n",
    "                        break\n",
    "            \n",
    "            # Cache and use the unique matches\n",
    "            all_matches = unique_matches\n",
    "            set_cached_response(cache_key, all_matches)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during restaurant search: {e}\", exc_info=True)\n",
    "            all_matches = []\n",
    "    \n",
    "    # Updates the state with the matches\n",
    "    state[\"restaurant_matches\"] = all_matches\n",
    "\n",
    "    # Get chat history\n",
    "    chat_history = []\n",
    "    if \"session_id\" in state and state[\"session_id\"]:\n",
    "        history = CONVERSATION_MEMORY.get_history(state[\"session_id\"], limit=3)\n",
    "        for item in history:\n",
    "            chat_history.append(HumanMessage(content=item[\"user_message\"]))\n",
    "            chat_history.append(AIMessage(content=item[\"bot_response\"]))\n",
    "\n",
    "    user_context = f\"\"\"\n",
    "        User query: {search_query}\n",
    "        \n",
    "        User's search criteria:\n",
    "        {search_criteria}\n",
    "        \n",
    "        Available restaurant matches:\n",
    "        {all_matches}  # Only using unique restaurant matches (maximum 3)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate a response using an LLM\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        SystemMessage(\n",
    "            content=\"\"\"\n",
    "                You are a restaurant recommendation assistant. Your task is to recommend restaurants based on the user's preferences and the retrieved restaurant data.\n",
    "                \n",
    "                Format your response precisely as follows:\n",
    "                1. Begin with a brief, friendly introduction (1-2 sentences only)\n",
    "                2. Present each restaurant recommendation as a numbered point\n",
    "                3. For each restaurant point, use this exact structure:\n",
    "                \n",
    "                ğŸ½ï¸ [RESTAURANT NAME]\n",
    "                â€¢ Cuisine: [cuisine type]\n",
    "                â€¢ Price: [price range]\n",
    "                â€¢ Notable features: [key features that match user preferences]\n",
    "                â€¢ Why it matches: [brief explanation of how it meets the user's criteria]\n",
    "                \n",
    "                4. End with a single, brief follow-up question about whether these recommendations are helpful.\n",
    "                \n",
    "                IMPORTANT: Do not recommend the same restaurant more than once, even if it appears multiple times in the data. Check restaurant \"id\" carefully and ensure each recommendation is for a unique restaurant. If you've already suggested a restaurant with a particular \"id\", do not suggest it again even if it has different details.\n",
    "\n",
    "                Keep your response concise and well-structured with clear formatting for easy readability.\n",
    "            \"\"\"\n",
    "        ),\n",
    "        *chat_history,\n",
    "        HumanMessage(content=user_context)\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        llm = get_llm(temperature=0.2)\n",
    "        chain = prompt | llm\n",
    "        \n",
    "        logger.info(\"Sending recommendation request to LLM\")\n",
    "        response = chain.invoke({})\n",
    "        logger.debug(f\"Received LLM response of length {len(response.content)}\")\n",
    "        \n",
    "        # Add the response to the messages\n",
    "        state[\"messages\"].append(AIMessage(content=response.content))\n",
    "        logger.info(\"Added restaurant recommendation response to state\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating restaurant recommendation: {e}\", exc_info=True)\n",
    "        error_msg = \"I'm sorry, I'm having trouble finding restaurant recommendations right now. Could you please try again or provide more details about what you're looking for?\"\n",
    "        state[\"messages\"].append(AIMessage(content=error_msg))\n",
    "    \n",
    "    return state \n",
    "\n",
    "def handle_restaurant_info(state: ChatState) -> ChatState:\n",
    "    \"\"\"\n",
    "    Handles queries about specific restaurants by searching for that restaurant\n",
    "    and providing detailed information\n",
    "    \n",
    "    Args:\n",
    "        state: The current chat state\n",
    "        \n",
    "    Returns:\n",
    "        Updated state with specific restaurant information\n",
    "    \"\"\"\n",
    "    \n",
    "    session_id = state.get(\"session_id\", \"unknown_session\")\n",
    "    logger.info(f\"Processing specific restaurant info for session {session_id}\")\n",
    "\n",
    "    last_message = state[\"messages\"][-1].content if state[\"messages\"] else \"\"\n",
    "    \n",
    "    # Build a query focused on the restaurant name\n",
    "    query_parts = [last_message]\n",
    "    \n",
    "    if state.get(\"specific_restaurant\"):\n",
    "        restaurant_names = state[\"specific_restaurant\"]\n",
    "        if isinstance(restaurant_names, list):\n",
    "            query_parts.append(f\"Restaurant name: {', '.join(restaurant_names)}\")\n",
    "            logger.debug(f\"Looking for specific restaurants: {', '.join(restaurant_names)}\")\n",
    "        else:\n",
    "            query_parts.append(f\"Restaurant name: {restaurant_names}\")\n",
    "            logger.debug(f\"Looking for specific restaurant: {restaurant_names}\")\n",
    "    \n",
    "    # Build the complete query\n",
    "    search_query = \" \".join(query_parts)\n",
    "    logger.info(f\"Built restaurant info query: {search_query[:100]}...\")\n",
    "    \n",
    "    # Set up retriever if not already done\n",
    "    retriever = setup_retriever_with_persistence(r\"C:\\Users\\Rithwik Khera\\OneDrive - iitr.ac.in\\Desktop\\assignment\\zeal\\100_restaurant_data.json\", r\"C:\\Users\\Rithwik Khera\\OneDrive - iitr.ac.in\\Desktop\\assignment\\zeal\\restaurant_idx\")  # Assuming the file path\n",
    "    \n",
    "    # Search for the restaurant\n",
    "    # Check cache first\n",
    "    cache_key = f\"info_{search_query}\"\n",
    "    cached_matches = get_cached_response(cache_key)\n",
    "    \n",
    "    if cached_matches:\n",
    "        logger.info(\"Using cached restaurant info matches\")\n",
    "        matches = cached_matches\n",
    "    else:\n",
    "        # Perform the search\n",
    "        logger.info(\"Performing vector search for specific restaurant info\")\n",
    "        # Retrieve more results initially to ensure we can find at least 3 unique restaurants\n",
    "        results = retriever.invoke(search_query, top_k=5)\n",
    "        logger.debug(f\"Retrieved {len(results)} results from vector search\")\n",
    "\n",
    "        # Tracking unique restaurant IDs to avoid duplicates using set data structure\n",
    "        seen_restaurant_ids = set()\n",
    "        unique_matches = []\n",
    "        \n",
    "        # Process the results\n",
    "        for doc in results:\n",
    "            metadata = doc.metadata\n",
    "            restaurant_id = metadata.get(\"id\", \"\")\n",
    "            \n",
    "            # Only add this restaurant if we haven't seen it before\n",
    "            if restaurant_id and restaurant_id not in seen_restaurant_ids:\n",
    "                seen_restaurant_ids.add(restaurant_id)\n",
    "                unique_matches.append({\n",
    "                    \"name\": metadata.get(\"name\", \"Unknown Restaurant\"),\n",
    "                    \"id\": restaurant_id,\n",
    "                    \"content\": doc.page_content,\n",
    "                    \"price\": metadata.get(\"price\"),\n",
    "                    \"restaurant_url\": metadata.get(\"restaurant_url\"),\n",
    "                    \"images_url\": metadata.get(\"images_url\"),\n",
    "                    \"coordinates\": metadata.get(\"coordinates\"),\n",
    "                    \"original_data\": metadata.get(\"original_data\", {})\n",
    "                })\n",
    "                \n",
    "                # Stop after finding 3 unique restaurants\n",
    "                if len(unique_matches) >= 3:\n",
    "                    break\n",
    "\n",
    "        # Cache and use the unique matches\n",
    "        matches = unique_matches\n",
    "        set_cached_response(cache_key, matches)\n",
    "            \n",
    "    # Update the state with the matches\n",
    "    state[\"restaurant_matches\"] = matches\n",
    "    \n",
    "    # Get chat history\n",
    "    chat_history = []\n",
    "    if \"session_id\" in state and state[\"session_id\"]:\n",
    "        history = CONVERSATION_MEMORY.get_history(state[\"session_id\"], limit=3)\n",
    "        for item in history:\n",
    "            chat_history.append(HumanMessage(content=item[\"user_message\"]))\n",
    "            chat_history.append(AIMessage(content=item[\"bot_response\"]))\n",
    "    \n",
    "    user_context = f\"\"\"\n",
    "        User query: {search_query}\n",
    "        \n",
    "        Search criteria: {state.get(\"specific_restaurant\", [])}\n",
    "        \n",
    "        Restaurant matches: {matches}  # Using all unique matches (maximum 3)\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate a response using an LLM\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        SystemMessage(\n",
    "            content=\"\"\"\n",
    "                You are a restaurant information assistant. Based on the user's query about a specific restaurant,\n",
    "                provide detailed information in a structured, point-by-point format.\n",
    "                \n",
    "                Format your response precisely as follows:\n",
    "                \n",
    "                If you can identify ONE specific restaurant the user is asking about:\n",
    "                \n",
    "                ğŸ½ï¸ [RESTAURANT NAME]\n",
    "                â€¢ Cuisine: [cuisine type]\n",
    "                â€¢ Price: [price range]\n",
    "                â€¢ Location: [location details]\n",
    "                â€¢ Highlights: [key features, specialties, or popular dishes]\n",
    "                â€¢ Hours: [if available]\n",
    "                â€¢ Contact: [if available]\n",
    "                â€¢ [Any other specific information the user requested]\n",
    "                \n",
    "                If MULTIPLE restaurants match and you're unsure which one:\n",
    "                1. Start with a brief note mentioning you found multiple matches\n",
    "                2. For each restaurant, provide a brief summary using the format above\n",
    "                3. Ask which specific restaurant they'd like more details about\n",
    "                \n",
    "                Keep your response concise with clear, consistent formatting and structure.\n",
    "            \"\"\"\n",
    "        ),\n",
    "        *chat_history,\n",
    "        HumanMessage(content=user_context)\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    llm = get_llm(temperature=0.2)\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({})\n",
    "    \n",
    "    # Adding the response to the messages\n",
    "    state[\"messages\"].append(AIMessage(content=response.content))\n",
    "    return state\n",
    "\n",
    "def handle_casual_conversation(state: ChatState) -> ChatState:\n",
    "    \"\"\"\n",
    "    Handles casual conversation with the user\n",
    "    \n",
    "    Args:\n",
    "        state: The current chat state\n",
    "        \n",
    "    Returns:\n",
    "        Updated state with a casual response\n",
    "    \"\"\"\n",
    "    last_message = state[\"messages\"][-1].content if state[\"messages\"] else \"\"\n",
    "\n",
    "    # Get chat history\n",
    "    chat_history = []\n",
    "    if \"session_id\" in state and state[\"session_id\"]:\n",
    "        history = CONVERSATION_MEMORY.get_history(state[\"session_id\"], limit=3)\n",
    "        for item in history:\n",
    "            chat_history.append(HumanMessage(content=item[\"user_message\"]))\n",
    "            chat_history.append(AIMessage(content=item[\"bot_response\"]))\n",
    "    \n",
    "    # Generate a casual response using an LLM\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        SystemMessage(\n",
    "            content=\"\"\"\n",
    "                You are a friendly restaurant assistant chatbot. Respond naturally to casual conversation,\n",
    "                greetings, thanks, or general questions. Be friendly, helpful, and conversational.\n",
    "                \n",
    "                If the conversation shifts to restaurants, pivot to offering structured help:\n",
    "                \n",
    "                \"I can help you find restaurants based on:\n",
    "                â€¢ Cuisine type\n",
    "                â€¢ Location\n",
    "                â€¢ Price range\n",
    "                â€¢ Special features (outdoor seating, vegan options, etc.)\n",
    "                \n",
    "                Just let me know what you're looking for!\"\n",
    "                \n",
    "                Keep casual responses brief and engaging. If the user is asking a non-restaurant question,\n",
    "                still be helpful but gently remind them that you specialize in restaurant recommendations\n",
    "                and information.\n",
    "            \"\"\"\n",
    "        ),\n",
    "        *chat_history,\n",
    "        HumanMessage(content=last_message)\n",
    "    ])\n",
    "    \n",
    "    # Use the LLM to generate a response\n",
    "    llm = get_llm(temperature=0.2)\n",
    "    chain = prompt | llm\n",
    "    \n",
    "    response = chain.invoke({})\n",
    "    \n",
    "    # Add the response to the messages\n",
    "    state[\"messages\"].append(AIMessage(content=response.content))\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main workflow graph definition\n",
    "def create_restaurant_assistant_graph() -> StateGraph:\n",
    "    \"\"\"\n",
    "    Creates the main workflow graph for the restaurant chatbot\n",
    "    \n",
    "    Returns:\n",
    "        A StateGraph object representing the workflow\n",
    "    \"\"\"\n",
    "    # Define the workflow\n",
    "    workflow = StateGraph(ChatState)\n",
    "    \n",
    "    # Add nodes to the graph\n",
    "    workflow.add_node(\"analyze_query\", analyze_user_query)\n",
    "    workflow.add_node(\"restaurant_recommendation\", handle_restaurant_recommendation)\n",
    "    workflow.add_node(\"restaurant_info\", handle_restaurant_info)\n",
    "    workflow.add_node(\"casual_conversation\", handle_casual_conversation)\n",
    "    \n",
    "    # Add edges\n",
    "    workflow.add_conditional_edges(\n",
    "        \"analyze_query\",\n",
    "        route_query,\n",
    "        {\n",
    "            \"restaurant_recommendation\": \"restaurant_recommendation\",\n",
    "            \"restaurant_info\": \"restaurant_info\",\n",
    "            \"casual_conversation\": \"casual_conversation\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Set completion paths\n",
    "    workflow.add_edge(\"restaurant_recommendation\", END)\n",
    "    workflow.add_edge(\"restaurant_info\", END)\n",
    "    workflow.add_edge(\"casual_conversation\", END)\n",
    "    \n",
    "    # Set the entry point\n",
    "    workflow.set_entry_point(\"analyze_query\")\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "# Create an application function to handle incoming messages\n",
    "def handle_message(message, session_id=None, stream=False):\n",
    "    \"\"\"\n",
    "    Handle an incoming message from a user\n",
    "    \n",
    "    Args:\n",
    "        message (str): The user's message\n",
    "        session_id (str, optional): A unique session identifier\n",
    "        stream (bool, optional): Whether to stream the response\n",
    "        \n",
    "    Returns:\n",
    "        If stream=False: str with the complete response\n",
    "        If stream=True: Generator that yields tokens one by one\n",
    "    \"\"\"\n",
    "    # Default session ID if none provided\n",
    "    if not session_id:\n",
    "        session_id = str(int(time.time()))\n",
    "    \n",
    "    # Initialize streaming queue if needed\n",
    "    response_queue = queue.Queue() if stream else None\n",
    "    \n",
    "    # Create or get the compiled graph\n",
    "    graph = create_restaurant_assistant_graph()\n",
    "    \n",
    "    # Initialize the state\n",
    "    state = ChatState(\n",
    "        messages=[HumanMessage(content=message)],\n",
    "        intent=None,\n",
    "        user_preferences={\"cuisine_type\": [], \"food_type\": [], \"location\": \"\", \"special_features\": []},\n",
    "        specific_restaurant=None,\n",
    "        restaurant_matches=None,\n",
    "        conversation_history=None,\n",
    "        session_id=session_id\n",
    "    )\n",
    "\n",
    "    # Get streaming LLM if streaming is requested\n",
    "    if stream:\n",
    "        # Override the get_llm function result in the global namespace\n",
    "        global get_llm\n",
    "        original_get_llm = get_llm\n",
    "        \n",
    "        def streaming_get_llm(*args, **kwargs):\n",
    "            return original_get_llm(streaming=True, queue=response_queue, *args, **kwargs)\n",
    "        \n",
    "        get_llm = streaming_get_llm # Temporarily replaces get_llm with our streaming version\n",
    "    \n",
    "    # Starts processing in a separate thread if streaming\n",
    "    if stream:\n",
    "        def response_generator(): # generator to yield streaming tokens\n",
    "            # Start a thread to process the request\n",
    "            def process_request():\n",
    "                nonlocal graph, state\n",
    "                try:\n",
    "                    result = graph.invoke(state)\n",
    "                    response_queue.put(None) # Signal completion\n",
    "                    \n",
    "                    # Get the final response for memory storage\n",
    "                    response = result[\"messages\"][-1].content if result[\"messages\"] else \"I'm not sure how to respond to that.\"\n",
    "                    \n",
    "                    CONVERSATION_MEMORY.add_interaction( # Stores the interaction in memory\n",
    "                        session_id=session_id,\n",
    "                        user_message=message,\n",
    "                        bot_response=response,\n",
    "                        metadata={\n",
    "                            \"intent\": result.get(\"intent\"),\n",
    "                            \"preferences\": result.get(\"user_preferences\")\n",
    "                        }\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error in streaming process: {e}\")\n",
    "                    response_queue.put(None)  # Signal completion even on error\n",
    "                \n",
    "                # Restore the original get_llm function\n",
    "                global get_llm\n",
    "                get_llm = original_get_llm\n",
    "                \n",
    "            # Start processing thread\n",
    "            threading.Thread(target=process_request).start()\n",
    "            \n",
    "            # Yield tokens as they arrive\n",
    "            while True:\n",
    "                token = response_queue.get()\n",
    "                if token is None:  # End of response\n",
    "                    break\n",
    "                yield token\n",
    "        \n",
    "        # Return the generator\n",
    "        return response_generator()\n",
    "\n",
    "    else:\n",
    "        # Non-streaming mode - execute synchronously\n",
    "        result = graph.invoke(state)\n",
    "        \n",
    "        # Get the final response\n",
    "        response = result[\"messages\"][-1].content if result[\"messages\"] else \"I'm not sure how to respond to that.\"\n",
    "        \n",
    "        # Store the interaction in memory\n",
    "        CONVERSATION_MEMORY.add_interaction(\n",
    "            session_id=session_id,\n",
    "            user_message=message,\n",
    "            bot_response=response,\n",
    "            metadata={\n",
    "                \"intent\": result.get(\"intent\"),\n",
    "                \"preferences\": result.get(\"user_preferences\")\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return response\n",
    "    \n",
    "# Example usage / testing function\n",
    "def test_assistant(stream=False):\n",
    "    \"\"\"Simple test function to demonstrate the assistant capabilities\"\"\"\n",
    "    session_id = str(int(time.time()))\n",
    "    test_queries = [\n",
    "        \"Show me great chinese restaurants in San Francisco\",\n",
    "        #\"What are great pasta restaurants in Philadelphia\", \n",
    "        \"Could you also suggest me good sushi places there?\",\n",
    "        \"Is reservation required in the above restaurants?\"\n",
    "    ]\n",
    "    \n",
    "    logger.info(\"ğŸ½ï¸ Testing Restaurant Assistant ğŸ½ï¸\\n\")\n",
    "    \n",
    "    for query in test_queries:\n",
    "        logger.info(f\"User: {query}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        if stream:\n",
    "            logger.info(\"Streaming response for query\")\n",
    "            print(\"Assistant: \", end=\"\", flush=True)  # Use print instead of logger.info for streaming output\n",
    "            response_parts = []\n",
    "            for token in handle_message(query, session_id, stream=True):\n",
    "                print(token, end=\"\", flush=True)\n",
    "                response_parts.append(token)\n",
    "            response = \"\".join(response_parts)\n",
    "        else:\n",
    "            response = handle_message(query, session_id)\n",
    "            logger.info(f\"Assistant: {response}\")\n",
    "            \n",
    "        end_time = time.time()\n",
    "        logger.info(f\"Assistant ({end_time - start_time:.2f}s): {response}\\n\")\n",
    "        time.sleep(1)  # Pause between queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage / testing function\n",
    "def test_assistant(stream=False):\n",
    "    \"\"\"Simple test function to demonstrate the assistant capabilities\"\"\"\n",
    "    session_id = str(int(time.time()))\n",
    "    test_queries = [\n",
    "        \"Show me great chinese restaurants in San Francisco\",\n",
    "        #\"What are great pasta restaurants in Philadelphia\", \n",
    "        \"Could you also suggest me good sushi places there?\",\n",
    "        \"Is reservation required in the above restaurants?\"\n",
    "    ]\n",
    "    \n",
    "    logger.info(\"ğŸ½ï¸ Testing Restaurant Assistant ğŸ½ï¸\\n\")\n",
    "    \n",
    "    for query in test_queries:\n",
    "        logger.info(f\"User: {query}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        if stream:\n",
    "            logger.info(\"Streaming response for query\")\n",
    "            print(\"Assistant: \", end=\"\", flush=True)  # Use print instead of logger.info for streaming output\n",
    "            response_parts = []\n",
    "            for token in handle_message(query, session_id, stream=True):\n",
    "                print(token, end=\"\", flush=True)\n",
    "                response_parts.append(token)\n",
    "            response = \"\".join(response_parts)\n",
    "        else:\n",
    "            response = handle_message(query, session_id)\n",
    "            logger.info(f\"Assistant: {response}\")\n",
    "            \n",
    "        end_time = time.time()\n",
    "        logger.info(f\"Assistant ({end_time - start_time:.2f}s): {response}\\n\")\n",
    "        time.sleep(1)  # Pause between queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 22:18:35,954 - __main__ - INFO - ğŸ½ï¸ Testing Restaurant Assistant ğŸ½ï¸\n",
      "\n",
      "2025-03-23 22:18:35,956 - __main__ - INFO - User: Show me great chinese restaurants in San Francisco\n",
      "2025-03-23 22:18:35,957 - __main__ - INFO - Streaming response for query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 22:18:36,063 - __main__ - INFO - Analyzing user query for session 1742748515\n",
      "2025-03-23 22:18:36,066 - __main__ - INFO - Sending query to LLM for analysis\n",
      "2025-03-23 22:18:36,806 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"intent\": \"restaurant_recommendation\",\n",
      "    \"extracted_info\": {\n",
      "        \"cuisine_type\": [\"Chinese\"],\n",
      "        \"food_type\": [],\n",
      "        \"location\": \"San Francisco\",\n",
      "        \"special_features\": [],\n",
      "        \"restaurant_names\": []\n",
      "    }\n",
      "}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 22:18:37,945 - __main__ - INFO - Classified intent: restaurant_recommendation\n",
      "2025-03-23 22:18:37,947 - __main__ - INFO - Analysis complete and cached\n",
      "2025-03-23 22:18:37,950 - __main__ - INFO - Routing query for session 1742748515 with intent: restaurant_recommendation\n",
      "2025-03-23 22:18:37,954 - __main__ - INFO - Processing restaurant recommendation for session 1742748515\n",
      "2025-03-23 22:18:37,955 - __main__ - INFO - Built search query: Show me great chinese restaurants in San Francisco Cuisine types: Chinese Location: San Francisco...\n",
      "2025-03-23 22:18:37,956 - __main__ - INFO - Found existing FAISS index at restaurant_idx\n",
      "2025-03-23 22:18:37,982 - __main__ - INFO - Loading FAISS index from restaurant_idx\n",
      "2025-03-23 22:18:37,989 - faiss.loader - INFO - Loading faiss with AVX2 support.\n",
      "2025-03-23 22:18:38,019 - faiss.loader - INFO - Successfully loaded faiss with AVX2 support.\n",
      "2025-03-23 22:18:38,028 - faiss - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes.\n",
      "2025-03-23 22:18:38,033 - __main__ - INFO - Successfully loaded FAISS index from restaurant_idx\n",
      "2025-03-23 22:18:38,034 - __main__ - INFO - Created and configured vector store retriever\n",
      "2025-03-23 22:18:38,034 - __main__ - INFO - Performing vector search for restaurants\n",
      "2025-03-23 22:18:39,007 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 22:18:39,050 - __main__ - INFO - Sending recommendation request to LLM\n",
      "2025-03-23 22:18:39,926 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some great Chinese restaurant recommendations in San Francisco that align with your search for delicious cuisine:\n",
      "\n",
      "1. **Taishan Cuisine**\n",
      "   - **Cuisine Type:** Cantonese, Noodles, Hot Pot\n",
      "   - **Location:** 785 Broadway, Chinatown, San Francisco, CA\n",
      "   - **Rating:** 3.9 (from 183 reviews)\n",
      "   - **Price Level:** Moderate\n",
      "   - **Key Features:** This cozy spot is perfect for late-night cravings, as itâ€™s open late. It offers a casual atmosphere and a variety of dishes, including noodles and hot pot, making it a great choice for those who enjoy a relaxed dining experience. The reviews highlight its inviting ambiance and comfort food appeal.\n",
      "   - **More Info:** [Taishan Cuisine on Yelp](https://www.yelp.com/reservations/taishan-cuisine-san-francisco)\n",
      "   - ![Taishan Cuisine](https://s3-media0.fl.yelpcdn.com/bphoto/cveoYYee-o-qjhs0Jhj9CQ/348s.jpg)\n",
      "\n",
      "2. **Hakka Restaurant å®¢å®¶å±±èŠ**\n",
      "   - **Cuisine Type:** Hakka (a style of Chinese cuisine)\n",
      "   - **Location:** 4401 Cabrillo St, San Francisco, CA\n",
      "   - **Key Features:** This restaurant offers a unique take on Chinese cuisine with Hakka dishes that are known for their rustic and comforting flavors. Signature dishes include salt-baked chicken and braised pork belly, served in family-style portions, making it ideal for sharing with friends or family. Itâ€™s a great place to experience something different from the typical Cantonese fare.\n",
      "   - **More Info:** [Hakka Restaurant](https://hakkarestaurantca.com/)\n",
      "   - ![Hakka Restaurant](https://cdn.vox-cdn.com/thumbor/zX9oErrkroe9At_WSdpN4LtOWtI=/0x0:1074x846/1200x900/filters:focal(452x338:622x508):no_upscale()/cdn.vox-cdn.com/uploads/chorus_image/image/71779595/Screenshot_2022_12_21_at_11.34.58_AM.0.png)\n",
      "\n",
      "3. **New Woey Loy Goey Restaurant**\n",
      "   - **Cuisine Type:** Cantonese\n",
      "   - **Location:** 699 Jackson St, San Francisco, CA\n",
      "   - **Key Features:** With a history of over 90 years, this basement restaurant in Chinatown is known for its hearty and affordable Cantonese dishes. Popular items include tomato-and-beef chow mein and salt-and-pepper fried pork chops. The prices are very reasonable, making it a great spot for a casual meal without breaking the bank.\n",
      "   - **More Info:** [New Woey Loy Goey](http://www.newwoeyloygoey.com/)\n",
      "   - ![New Woey Loy Goey](https://sf.eater.com/maps/best-restaurants-chinatown-dim-sum)\n",
      "\n",
      "These restaurants not only offer authentic Chinese cuisine but also provide a variety of dining experiences, from casual to unique Hakka flavors. Do any of these sound appealing to you, or would you like more suggestions?"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 22:18:52,736 - __main__ - INFO - Added restaurant recommendation response to state\n",
      "2025-03-23 22:18:52,741 - __main__ - INFO - Created new session: 1742748515\n",
      "2025-03-23 22:18:52,741 - __main__ - INFO - Assistant (16.78s): {\n",
      "    \"intent\": \"restaurant_recommendation\",\n",
      "    \"extracted_info\": {\n",
      "        \"cuisine_type\": [\"Chinese\"],\n",
      "        \"food_type\": [],\n",
      "        \"location\": \"San Francisco\",\n",
      "        \"special_features\": [],\n",
      "        \"restaurant_names\": []\n",
      "    }\n",
      "}Here are some great Chinese restaurant recommendations in San Francisco that align with your search for delicious cuisine:\n",
      "\n",
      "1. **Taishan Cuisine**\n",
      "   - **Cuisine Type:** Cantonese, Noodles, Hot Pot\n",
      "   - **Location:** 785 Broadway, Chinatown, San Francisco, CA\n",
      "   - **Rating:** 3.9 (from 183 reviews)\n",
      "   - **Price Level:** Moderate\n",
      "   - **Key Features:** This cozy spot is perfect for late-night cravings, as itâ€™s open late. It offers a casual atmosphere and a variety of dishes, including noodles and hot pot, making it a great choice for those who enjoy a relaxed dining experience. The reviews highlight its inviting ambiance and comfort food appeal.\n",
      "   - **More Info:** [Taishan Cuisine on Yelp](https://www.yelp.com/reservations/taishan-cuisine-san-francisco)\n",
      "   - ![Taishan Cuisine](https://s3-media0.fl.yelpcdn.com/bphoto/cveoYYee-o-qjhs0Jhj9CQ/348s.jpg)\n",
      "\n",
      "2. **Hakka Restaurant å®¢å®¶å±±èŠ**\n",
      "   - **Cuisine Type:** Hakka (a style of Chinese cuisine)\n",
      "   - **Location:** 4401 Cabrillo St, San Francisco, CA\n",
      "   - **Key Features:** This restaurant offers a unique take on Chinese cuisine with Hakka dishes that are known for their rustic and comforting flavors. Signature dishes include salt-baked chicken and braised pork belly, served in family-style portions, making it ideal for sharing with friends or family. Itâ€™s a great place to experience something different from the typical Cantonese fare.\n",
      "   - **More Info:** [Hakka Restaurant](https://hakkarestaurantca.com/)\n",
      "   - ![Hakka Restaurant](https://cdn.vox-cdn.com/thumbor/zX9oErrkroe9At_WSdpN4LtOWtI=/0x0:1074x846/1200x900/filters:focal(452x338:622x508):no_upscale()/cdn.vox-cdn.com/uploads/chorus_image/image/71779595/Screenshot_2022_12_21_at_11.34.58_AM.0.png)\n",
      "\n",
      "3. **New Woey Loy Goey Restaurant**\n",
      "   - **Cuisine Type:** Cantonese\n",
      "   - **Location:** 699 Jackson St, San Francisco, CA\n",
      "   - **Key Features:** With a history of over 90 years, this basement restaurant in Chinatown is known for its hearty and affordable Cantonese dishes. Popular items include tomato-and-beef chow mein and salt-and-pepper fried pork chops. The prices are very reasonable, making it a great spot for a casual meal without breaking the bank.\n",
      "   - **More Info:** [New Woey Loy Goey](http://www.newwoeyloygoey.com/)\n",
      "   - ![New Woey Loy Goey](https://sf.eater.com/maps/best-restaurants-chinatown-dim-sum)\n",
      "\n",
      "These restaurants not only offer authentic Chinese cuisine but also provide a variety of dining experiences, from casual to unique Hakka flavors. Do any of these sound appealing to you, or would you like more suggestions?\n",
      "\n",
      "2025-03-23 22:18:53,750 - __main__ - INFO - User: Could you also suggest me good sushi places there?\n",
      "2025-03-23 22:18:53,752 - __main__ - INFO - Streaming response for query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 22:18:53,765 - __main__ - INFO - Analyzing user query for session 1742748515\n",
      "2025-03-23 22:18:53,769 - __main__ - INFO - Sending query to LLM for analysis\n",
      "2025-03-23 22:18:54,746 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 22:18:55,858 - __main__ - INFO - Classified intent: restaurant_recommendation\n",
      "2025-03-23 22:18:55,860 - __main__ - INFO - Analysis complete and cached\n",
      "2025-03-23 22:18:55,864 - __main__ - INFO - Routing query for session 1742748515 with intent: restaurant_recommendation\n",
      "2025-03-23 22:18:55,866 - __main__ - INFO - Processing restaurant recommendation for session 1742748515\n",
      "2025-03-23 22:18:55,867 - __main__ - INFO - Built search query: Could you also suggest me good sushi places there? Cuisine types: Japanese Food types: sushi Locatio...\n",
      "2025-03-23 22:18:55,868 - __main__ - INFO - Performing vector search for restaurants\n",
      "2025-03-23 22:18:57,608 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 22:18:57,611 - __main__ - INFO - Sending recommendation request to LLM\n",
      "2025-03-23 22:18:58,445 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 22:19:12,154 - __main__ - INFO - Added restaurant recommendation response to state\n",
      "2025-03-23 22:19:12,157 - __main__ - INFO - Assistant (18.40s): \n",
      "\n",
      "2025-03-23 22:19:13,164 - __main__ - INFO - User: Is reservation required in the above restaurants?\n",
      "2025-03-23 22:19:13,165 - __main__ - INFO - Streaming response for query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 22:19:13,177 - __main__ - INFO - Analyzing user query for session 1742748515\n",
      "2025-03-23 22:19:13,180 - __main__ - INFO - Sending query to LLM for analysis\n",
      "2025-03-23 22:19:13,917 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 22:19:14,855 - __main__ - INFO - Classified intent: specific_restaurant_info\n",
      "2025-03-23 22:19:14,856 - __main__ - INFO - Analysis complete and cached\n",
      "2025-03-23 22:19:14,858 - __main__ - INFO - Routing query for session 1742748515 with intent: specific_restaurant_info\n",
      "2025-03-23 22:19:14,861 - __main__ - INFO - Processing specific restaurant info for session 1742748515\n",
      "2025-03-23 22:19:14,861 - __main__ - INFO - Built restaurant info query: Is reservation required in the above restaurants? Restaurant name: Taishan Cuisine, Hakka Restaurant...\n",
      "2025-03-23 22:19:14,862 - __main__ - INFO - Performing vector search for specific restaurant info\n",
      "2025-03-23 22:19:15,666 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 22:19:16,575 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-23 22:19:19,731 - __main__ - INFO - Assistant (6.57s): \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    test_assistant(stream=True) # runs the test function to demonstrate the assistant capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
